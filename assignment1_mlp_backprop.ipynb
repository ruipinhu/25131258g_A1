{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb76c89",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"assignment1_mlp_backprop.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_spiral_data(N=100, K=3):\n",
    "    \"\"\"\n",
    "    Generates a spiral dataset for classification.\n",
    "\n",
    "    Args:\n",
    "        N: Number of points per class\n",
    "        K: Number of classes\n",
    "    Returns:\n",
    "        X: Feature matrix (N*K, 2)\n",
    "        y: Labels (N*K,)\n",
    "    \"\"\"\n",
    "    D = 2 # Dimensionality\n",
    "    X = np.zeros((N*K,D))\n",
    "    y = np.zeros(N*K, dtype='uint8')\n",
    "    for j in range(K):\n",
    "      ix = range(N*j,N*(j+1))\n",
    "      r = np.linspace(0.0,1,N) # Radius\n",
    "      t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # Theta\n",
    "      X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "      y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Helper function to visualize the decision boundary of the model.\n",
    "    It creates a meshgrid, predicts the class for every point on the grid,\n",
    "    and plots the countours.\n",
    "    \"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7cb788",
   "metadata": {},
   "source": [
    "# Assignment 1: Building a Neural Network from Scratch\n",
    "\n",
    "In this assignment, you will implement the core components of a Multi-Layer Perceptron (MLP).\n",
    "\n",
    "## Objectives\n",
    "1.  Implement **Linear Layers**\n",
    "2.  Implement **ReLU** and **Tanh**\n",
    "3.  Implement **Softmax Cross Entropy Loss**\n",
    "4.  Assemble a **Two-Layer Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d6c35",
   "metadata": {},
   "source": [
    "# Part 1: The Linear Layer\n",
    "\n",
    "The Linear layer performs: $$ Y = XW + b $$\n",
    "\n",
    "Please implement the `forward` and `backward` methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5808f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Small random initialization to break symmetry\n",
    "        # We multiply by 0.01 to keep initial weights small\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.b = np.zeros(output_dim)\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.x = None \n",
    "        # Gradients (to be computed in backward)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the linear transformation: Y = XW + b\n",
    "\n",
    "        Args:\n",
    "            x: Input data of shape (N, input_dim)\n",
    "        Returns:\n",
    "            out: Output of shape (N, output_dim)\n",
    "        \"\"\"\n",
    "        self.x = x # Save input for backward pass\n",
    "        ...\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Computes gradients of loss w.r.t W, b, and input x.\n",
    "\n",
    "        Args:\n",
    "            dout: Upstream gradient of shape (N, output_dim)\n",
    "                  (Gradient of Loss w.r.t the output of this layer)\n",
    "        Returns:\n",
    "            dx: Gradient w.r.t input x, shape (N, input_dim)\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ec3fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8e58f",
   "metadata": {},
   "source": [
    "# Part 2: Activation Functions\n",
    "\n",
    "We need non-linearity to learn complex patterns. You will implement **ReLU** and **Tanh**.\n",
    "\n",
    "### ReLU\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "### Tanh\n",
    "$$ f(x) = \\tanh(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262ffa0",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ReLU(x) = max(0, x)\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        If input > 0, gradient passes through (1).\n",
    "        If input <= 0, gradient is killed (0).\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab669ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f8d42",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        d/dx tanh(x) = 1 - tanh(x)^2\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b646c7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5af75c",
   "metadata": {},
   "source": [
    "# Part 3: Softmax Cross Entropy Loss\n",
    "\n",
    "We combine Softmax and Cross Entropy for numerical stability.\n",
    "\n",
    "**Forward:**\n",
    "1. Shift logits: `logits - max(logits)` (prevents overflow).\n",
    "2. Softmax: `exp(logits) / sum(exp(logits))`.\n",
    "3. Loss: negative log likelihood of the correct class.\n",
    "\n",
    "**Backward:**\n",
    "$$ \\frac{\\partial L}{\\partial z} = \\frac{1}{N} (P - Y_{onehot}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f5525",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, logits, y):\n",
    "        \"\"\"\n",
    "        Computes the Softmax Cross Entropy Loss.\n",
    "\n",
    "        Args:\n",
    "            logits: Unnormalized scores from the last layer. Shape (N, C)\n",
    "            y: Ground truth labels (Integers). Shape (N,)\n",
    "        \"\"\"\n",
    "        N = logits.shape[0]\n",
    "\n",
    "        ...\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.probs = probs\n",
    "        self.y = y\n",
    "        self.N = N\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns gradient of Loss w.r.t logits (dx)\n",
    "        Shape: (N, C)\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7eb5c5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d4029",
   "metadata": {},
   "source": [
    "# Part 4: Putting it Together (TwoLayerNet)\n",
    "\n",
    "Now, assemble the components into a network.\n",
    "Architecture: `Linear -> Activation -> Linear -> SoftmaxLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a40209",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation='relu', lr=1e-1):\n",
    "        \"\"\"\n",
    "        Architecture:\n",
    "        Input -> Linear(1) -> Activation -> Linear(2) -> SoftmaxLoss\n",
    "        \"\"\"\n",
    "        self.fc1 = Linear(input_dim, hidden_dim)\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.act = ReLU()\n",
    "        else:\n",
    "            self.act = Tanh()\n",
    "\n",
    "        self.fc2 = Linear(hidden_dim, output_dim)\n",
    "        self.criterion = SoftmaxCrossEntropy()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "    def step(self, x, y):\n",
    "        \"\"\"\n",
    "        Performs one step of Gradient Descent.\n",
    "        1. Forward pass\n",
    "        2. Backward pass (Compute gradients)\n",
    "        3. Parameter update\n",
    "        Returns:\n",
    "            loss: scalar value\n",
    "        \"\"\"\n",
    "        # 1. Forward\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion.forward(logits, y)\n",
    "\n",
    "        # 2. Backward (Chain Rule)\n",
    "        # Note: We traverse the layers in reverse order\n",
    "        ...\n",
    "\n",
    "        # 3. Update (SGD)\n",
    "        # W = W - learning_rate * dW\n",
    "        ...\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d415ca",
   "metadata": {},
   "source": [
    "# Part 5: Training on Spiral Data\n",
    "\n",
    "Now we train the model to classify the spiral dataset. If your gradient implementation is correct, the accuracy should reach > 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Data\n",
    "X, y = generate_spiral_data(N=100, K=3)\n",
    "\n",
    "# 2. Initialize Model\n",
    "model = TwoLayerNet(input_dim=2, hidden_dim=100, output_dim=3, activation='relu', lr=1.0)\n",
    "\n",
    "# 3. Training Loop\n",
    "print(\"Training started...\")\n",
    "for i in range(2000):\n",
    "    loss = model.step(X, y)\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        predictions = np.argmax(model.forward(X), axis=1)\n",
    "        acc = np.mean(predictions == y)\n",
    "        print(f\"Iter {i}: Loss {loss:.4f}, Acc {acc:.2f}\")\n",
    "\n",
    "# 4. Visualize\n",
    "print(\"Final Decision Boundary:\")\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82487b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53336ab8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea5c92",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> (N, Din, Dout) = (2, 3, 2)\n>>> x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n>>> layer = Linear(Din, Dout)\n>>> layer.W = np.ones((Din, Dout))\n>>> layer.b = np.zeros(Dout)\n>>> out = layer.forward(x)\n>>> assert np.allclose(out, [[6.0, 6.0], [15.0, 15.0]]), 'Forward pass incorrect'\n>>> dout = np.ones((N, Dout))\n>>> dx = layer.backward(dout)\n>>> assert np.allclose(layer.dW, [[5.0, 5.0], [7.0, 7.0], [9.0, 9.0]]), 'dW incorrect'\n>>> assert np.allclose(layer.db, [2.0, 2.0]), 'db incorrect'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> relu = ReLU()\n>>> x = np.array([-1.0, 2.0, 0.0])\n>>> out = relu.forward(x)\n>>> assert np.allclose(out, [0.0, 2.0, 0.0]), 'ReLU Forward failed'\n>>> dout = np.array([5.0, 5.0, 5.0])\n>>> dx = relu.backward(dout)\n>>> assert np.allclose(dx, [0.0, 5.0, 0.0]), 'ReLU Backward failed'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> tanh = Tanh()\n>>> x = np.array([0.0])\n>>> out = tanh.forward(x)\n>>> assert np.allclose(out, [0.0]), 'Tanh Forward failed'\n>>> dout = np.array([10.0])\n>>> dx = tanh.backward(dout)\n>>> assert np.allclose(dx, [10.0]), 'Tanh Backward failed'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> criterion = SoftmaxCrossEntropy()\n>>> logits = np.array([[0.0, 100.0]])\n>>> y = np.array([1])\n>>> loss = criterion.forward(logits, y)\n>>> assert np.isclose(loss, 0.0), 'Loss should be near 0 for perfect prediction'\n>>> dx = criterion.backward()\n>>> assert dx.shape == logits.shape, 'Gradient shape mismatch'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
